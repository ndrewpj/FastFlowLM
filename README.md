# ğŸš€ FastFlowLM

**Deploy large language models (LLMs) on AMD Ryzenâ„¢ AI NPUsâ€”in minutes.**

Think **Ollama**, but purpose-built for the great **AMD Ryzenâ„¢ NPUs**.

---

## ğŸ”§ What is FastFlowLM?

**FastFlowLM** is a high-performance runtime for deploying popular LLMs like **LLaMA** and **DeepSeek-R1** directly on AMD Ryzenâ„¢ NPUs. It's hardware-optimized for lightning-fast, low-power, private, always-on AIâ€”running entirely on the silicon already in your AI PC.

---

## ğŸ‘¨â€ğŸ’» Built for Developers Building Local AI Agents

### ğŸ§  Zero Low-Level Tuning
You don't need to know anything about NPU internalsâ€”just run your model. FastFlowLM takes care of all the hardware optimization for you.

### ğŸ§° Familiar Workflow
FastFlowLM offers the **CLI and API simplicity** developers love from tools like Ollama, but optimized for **AMD Ryzenâ„¢ NPU** execution.

### ğŸ’» No GPU or CPU Required
Models run **entirely on the Ryzenâ„¢ NPU**, leaving your CPU and GPU free for other tasks.

### ğŸ“ Full Context Window Support  
Supports full-context windowsâ€”**up to 128k tokens** on models like **LLaMA 3.1/3.2**â€”ideal for long-form reasoning and RAG applications.

---

## âš¡ Performance That Speaks for Itself

Compared to AMD Ryzenâ„¢ AI Software 1.4 (GAIA or Lemonade):

### ğŸš€ LLM Decoding Speed *(TPS: Tokens per Second)*
- Up to **14.2Ã— faster** in LLM decoding *(vs NPU-only baseline)*
- Up to **16.2Ã— faster** in LLM decoding *(vs hybrid iGPU+NPU baseline)*

### ğŸ”‹ Power Efficiency
- Up to **2.66Ã— more efficient** in LLM decoding *(vs NPU-only baseline)*
- Up to **11.38Ã— more efficient** in LLM decoding *(vs hybrid iGPU+NPU baseline)*
- Up to **3.4Ã— more efficient** in LLM prefill *(vs NPU-only or hybrid baseline)*

### â±ï¸ Latency *(LLM Prefill Speed)*
- **Matches or exceeds** the **Time to First Token (TTFT)** of **NPU-only** or **hybrid** baselines

---

## ğŸš€ Get Started

Coming soon: setup instructions, model loading guide, and API examples.

---

## ğŸ§ª Supported Models

- Meta LLaMA 2 / 3
- DeepSeek-V2 / R1
- Phi-2 / Phi-3
- Command-R / Zephyr
- And more...

---

## ğŸ“„ License

MIT License

---

## ğŸ™Œ Acknowledgments

Thanks to AMD for Ryzenâ„¢ AI hardware innovation, and the open-source community for continued support.

---
