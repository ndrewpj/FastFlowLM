<p align="center">
  <a href="https://www.fastflowlm.com" target="_blank">
    <img src="assets/logo.png" alt="FastFlowLM Logo" width="200"/>
  </a>
</p>


Run large language models on AMD Ryzenâ„¢ AI NPUs â€” in minutes.

FastFlowLM is a lightweight runtime for deploying LLMs like LLaMA and DeepSeek directly on AMDâ€™s integrated NPU â€” no GPU or CPU needed.

**Just like Ollama â€” but built for the great Ryzenâ„¢ NPU.**

---

## ğŸ“º Demo Videos

FastFlowLM vs AMDâ€™s official stack â€” **real-time speedup and power efficiency**: 

- Same prompt (length: 1835 tokens), same model (LLaMA 3.2 1B model), running on the same machine (AMD Ryzen AI 5 340 NPU with 32GB SO-DIMM DDR5 5600 MHz memory)
- Real-time CPU, iGPU, NPU usage, and power consumption shown (Windows task manager + HWINFO)

### ğŸ”¹ FastFlowLM vs AMD Ryzen AI Software 1.4 (NPU-only via OGA)

[![Demo: FastFlowLM vs OGA](https://img.youtube.com/vi/kv31FZ_q0_I/0.jpg)](https://www.youtube.com/watch?v=kv31FZ_q0_I)

### ğŸ”¹ FastFlowLM vs AMD Ryzen AI Software 1.4 (Hybrid iGPU+NPU via GAIA)

[![Demo: FastFlowLM vs GAIA](https://img.youtube.com/vi/PFjH-L_Kr0w/0.jpg)](https://www.youtube.com/watch?v=PFjH-L_Kr0w)

---

## ğŸ§  Local AI on Your NPU

FastFlowLM makes it easy to run modern LLMs locally with:
- âš¡ High performance and low power
- ğŸ§° Simple CLI and API
- ğŸ” Fully private and offline

No drivers, no model rewrites, no tuning â€” it just works.

---

## âœ… Features

- **Runs fully on AMD Ryzenâ„¢ NPU** â€” no GPU or CPU load  
- **CLI-first developer flow** â€” like Ollama, but optimized for NPU  
- **Support for long context windows** â€” up to 128k tokens (e.g., LLaMA 3.1/3.2)  
- **No low-level tuning required** â€” Worry about your app, we handle the rest

---

## âš¡ Performance

Compared to AMD Ryzenâ„¢ AI Software 1.4 (GAIA or Lemonade):

### LLM Decoding Speed (TPS: Tokens per Second)
- ğŸš€ Up to **14.2Ã— faster** vs NPU-only baseline  
- ğŸš€ Up to **16.2Ã— faster** vs hybrid iGPU+NPU baseline

### Power Efficiency
- ğŸ”‹ Up to **2.66Ã— more efficient** vs NPU-only  
- ğŸ”‹ Up to **11.38Ã— more efficient** vs hybrid  
- ğŸ”‹ Up to **3.4Ã— more efficient in prefill** vs NPU-only or hybrid

### Latency
- â±ï¸ **Matches or exceeds** TTFT of NPU-only or hybrid configurations

---

## ğŸ§ª Model Support

FastFlowLM supports many of todayâ€™s best open models:
- LLaMA 3.1 / 3.2  
- DeepSeek R1  
*...with more models coming soon.*

---

## ğŸ› ï¸ Getting Started

Documentation, install guides, and example workflows coming soon.  
Youâ€™ll be able to:
- Load and run models locally via CLI
- Integrate into your app via a simple HTTP API

---

## ğŸ”’ Proprietary Kernel Optimizations

FastFlowLM uses **proprietary low-level kernel code** optimized for AMD Ryzenâ„¢ NPUs.  
> These kernels are **not open source**, but are included as binaries for seamless integration.

The rest of the stack â€” CLI, model runner, orchestration â€” is open and developer-friendly.

---

## ğŸ“ Licensing & Contact

- ğŸ†“ **Deep-optimized FastFlowLM models** are **free for non-commercial use**  
- ğŸ’¼ **Interested in commercial licensing?** Email us at [info@fastflowlm.edu](mailto:info@fastflowlm.edu)  
- ğŸ“¦ **Want to bring your own model?** We can optimize it for FastFlowLM â€” just reach out!

---

## License

Open components are released under the **MIT License**. Proprietary binaries are subject to separate terms.

---

ğŸ’¬ **Have feedback or want early access? [Open an issue](#) or reach out!**
