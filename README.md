# ðŸš€ FastFlowLM

**Deploy large language models (LLMs) on AMD Ryzenâ„¢ AI NPUsâ€”in minutes.**  
Think **Ollama**, purpose-built for the AMD Ryzenâ„¢ NPU architecture.

---

## Overview

**FastFlowLM** is a high-performance runtime for deploying state-of-the-art LLMsâ€”such as **LLaMA**, **DeepSeek**, and othersâ€”directly on AMD Ryzenâ„¢ NPUs. It is engineered for **low-latency**, **low-power**, and **always-on** AI, leveraging the NPU silicon already embedded in next-gen AI PCs.

---

## Key Features

### ðŸ‘¨â€ðŸ’» Developer-Centric Design  
Purpose-built for local AI agent development.  
No low-level NPU knowledge requiredâ€”FastFlowLM abstracts the hardware layer for you.

### ðŸ§° Ollama-Like Simplicity  
Offers a streamlined **CLI** and **API**, mirroring the ease of use of Ollama, with the added benefit of **native NPU acceleration**.

### ðŸ’» Zero GPU/CPU Dependency  
All model inference runs entirely on the **AMD Ryzenâ„¢ NPU**, freeing CPU and GPU resources for other workloads.

### ðŸ“ Extended Context Support  
Supports long context windowsâ€”**up to 128k tokens** on models like **LLaMA 3.1/3.2**â€”enabling long-form reasoning, multi-turn memory, and RAG workflows without compromise.

---

## âš¡ Performance That Speaks for Itself

Compared to AMD Ryzenâ„¢ AI Software 1.4 (GAIA or Lemonade):

### ðŸš€ LLM Decoding Throughput *(TPS: Tokens per Second)*
- Up to **14.2Ã— faster** vs NPU-only baseline  
- Up to **16.2Ã— faster** vs hybrid iGPU+NPU baseline

### ðŸ”‹ Power Efficiency
- Up to **2.66Ã— more efficient** in LLM decoding vs NPU-only  
- Up to **11.38Ã— more efficient** in LLM decoding vs hybrid iGPU+NPU  
- Up to **3.4Ã— more efficient** in LLM prefill vs NPU-only or hybrid

### â±ï¸ Latency *(LLM Prefill Speed)*
- **Matches or exceeds** the **Time to First Token (TTFT)** performance of NPU-only and hybrid configurations

---

## Model Support

- âœ… Meta LLaMA 3.1 / 3.2  
- âœ… DeepSeek R1  
- âœ… And more...

---

## Quick Start (Coming Soon)

Full installation guide, API reference, and deployment examples will be published shortly.

---

## License

This project is released under the **MIT License**.  
Proprietary components are distributed in binary form and subject to separate licensing terms.

---

## ðŸ”’ Proprietary Kernel Optimizations

FastFlowLM leverages **proprietary, low-level kernel code** optimized specifically for AMD Ryzenâ„¢ NPUs.  
> These performance-critical components are **not open source**, but seamlessly integrated into the runtime for maximum efficiency and security.

The open-source layers include the CLI, model orchestration, and runtime logicâ€”enabling developers to integrate and deploy models without concern for NPU internals.

---

## Acknowledgments

Special thanks to **AMD Ryzenâ„¢ AI** engineering teams and the broader open-source community for driving innovation in efficient edge AI.

---


