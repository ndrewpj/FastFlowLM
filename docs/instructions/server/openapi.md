---
title: OpenAI API Standard
nav_order: 1
parent: Local Server (Server Mode)
---

## ğŸ“¡ How Does the OpenAI Standard Work?

The **OpenAI API standard** defines a simple and powerful way for applications to communicate with language models â€” whether running in the cloud or locally.

It uses a **multi-role chat format**, which includes three types of messages:

| **Role**      | **Description** |
|---------------|-----------------|
| ğŸ›  **System**     | Used to give high-level instructions to the model. For example: "You are a helpful assistant," or "You are a coding tutor that only explains using examples." It sets the tone, rules, or available tools for the model. |
| ğŸ‘¤ **User**       | Messages from the application to the model. These often come from the end-user (e.g., a typed prompt like: â€œExplain black holes.â€). |
| ğŸ¤– **Assistant**  | Responses generated by the model and returned to the application. These are answers to the user prompts. |

This structure makes it easy to build multi-turn conversations with consistent behavior.

---

### ğŸ“š Developer Support

OpenAI provides [official libraries](https://platform.openai.com/docs/libraries/python-library#install-an-official-sdk) in multiple programming languages to help developers follow the standard easily: **Python**, **JavaScript**, **.NET**, **Java**, and **Go**.

These libraries make it easy to send prompts, receive completions, and integrate with local or cloud-based OpenAI-compatible servers.

---

## ğŸš€ Quick Test: Use OpenAI SDK with FastFlowLM in Python

You can try this instantly in any Python environment â€” including Jupyter Notebook. Follow the steps below by copying each block into a notebook cell.

---

### âœ… Step 0: Start FastFlowLM in Server Mode

Open PowerShell or terminal and launch the model server:

```
flm serve llama3.2:1B
```

> ğŸ§  This loads the model and starts the FastFlowLM OpenAI-compatible API at `http://localhost:11434/v1`.

---

### âœ… Step 1: Install the OpenAI Python SDK

```python
!pip install --upgrade openai
```

---

### âœ… Step 2: Send a Chat Request to FastFlowLM

```python
from openai import OpenAI

# Connect to local FastFlowLM server
client = OpenAI(
    base_url="http://localhost:11434/v1",  # FastFlowLM's local API endpoint
    api_key="flm"  # Dummy key (FastFlowLM doesnâ€™t require authentication)
)

# Send a chat-style prompt using OpenAI API format
response = client.chat.completions.create(
    model="llama3.2:1B",  # Replace with any model you've launched with `flm serve`
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Why is the sky blue?"}
    ]
)

# Show the model's response
print(response.choices[0].message.content)
```

---

### ğŸ“Œ Notes

- ğŸ§  You can replace `"llama3.2:1B"` with any other model available via `flm run` or `flm pull`.
- ğŸ–¥ Make sure the FastFlowLM server is running in the background (`flm serve ...`).
- ğŸ”’ No real API key is needed â€” just pass `"flm"` as a placeholder.
- âš¡ FastFlowLM runs fully offline and is optimized for AMD Ryzen AI NPUs.

> âœ… This setup is perfect for quick offline LLM testing using standard OpenAI tooling.

---

## ğŸ§ª More Examples

ğŸš€ Ah â€” that was easy, right?  
Now letâ€™s kick things up a notch with some awesome next-level examples!

---

### ğŸ’¬ Example: Multi-turn Chat (Conversation History)

Use this pattern when you want the model to remember previous turns in the conversation:

```python
messages = [
    {"role": "system", "content": "You are a creative writing assistant."},
    {"role": "user", "content": "Write the beginning of a fantasy story."},
]

client = OpenAI(base_url="http://localhost:11434/v1", api_key="flm")
response = client.chat.completions.create(model="llama3.2:1B", messages=messages)
print(response.choices[0].message.content)

# Add the assistant response and continue the conversation
messages.append({"role": "assistant", "content": response.choices[0].message.content})
messages.append({"role": "user", "content": "Continue the story with a twist."})
response = client.chat.completions.create(model="llama3.2:1B", messages=messages)

print(response.choices[0].message.content)
```

> âš ï¸ The OpenAI API (and FastFlowLM server mode) is **stateless** â€” you must resend the full conversation each time. No KV cache is kept between turns.

> ğŸŒ€ This means all previous messages are reprocessed (**prefill**), which adds latency for long chats.

> âš¡ **FastFlowLMâ€™s CLI mode** uses a **real KV cache**, making multi-turn responses much faster â€” especially with long conversations.

> ğŸ§  FastFlowLM is optimized for **long sequences** with large KV caches, ideal for 32kâ€“128k context windows.

> ğŸ”§ Weâ€™re working on adding **stateful KV cache** to server mode. Stay tuned!

---

### ğŸ” Example: Streamed Output (Real-Time Response)

Display the modelâ€™s output as it generates, token-by-token:

```python
client = OpenAI(base_url="http://localhost:11434/v1", api_key="flm")

stream = client.chat.completions.create(
    model="llama3.2:1B",
    messages=[
        {"role": "system", "content": "You are a fast, concise assistant."},
        {"role": "user", "content": "Summarize the plot of Hamlet in 3 sentences."}
    ],
    stream=True
)

for chunk in stream:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="", flush=True)
```

---

### ğŸ“„ Example: Use a File as the Prompt

You can load a full `.txt` file as a prompt â€” useful for long documents or testing large context windows.

ğŸ‘‰ [Download the sample prompt](https://github.com/FastFlowLM/FastFlowLM/blob/main/assets/alice_in_wonderland.txt)  

Download to Downloads folder. This contains over 38k token, thus may take longer to prompt. FastFlowLM supports full context length (32kâ€“128k), making it ideal for this kind of task.

```python
with open("C:\\Users\\<username>\\Downloads\\alice_in_wonderland.txt", "r", encoding="utf-8") as f:
    user_prompt = f.read()

client = OpenAI(base_url="http://localhost:11434/v1", api_key="flm")
response = client.chat.completions.create(
    model="llama3.2:1B",
    messages=[
        {"role": "system", "content": "You are a precise assistant."},
        {"role": "user", "content": user_prompt}
    ]
)

print(response.choices[0].message.content)
```

---

### ğŸ“Š Example: Batch Requests (Multiple Prompts)

Loop over a list of prompts and generate answers â€” useful for eval or bulk testing.

```python
prompts = [
    "Summarize the causes of World War I.",
    "Describe how a transistor works.",
    "What are the key themes in â€˜To Kill a Mockingbirdâ€™?",
]

for prompt in prompts:
    response = client.chat.completions.create(
        model="llama3.2:1B",
        messages=[
            {"role": "system", "content": "You are a concise academic tutor."},
            {"role": "user", "content": prompt}
        ]
    )
    print(f"\nğŸ“ Prompt: {prompt}\nğŸ” Answer: {response.choices[0].message.content}")
```

---

### ğŸ§¬ Example: Use Temperature, Top-p, and Presence Penalty

Control randomness and creativity â€” for brainstorming or open-ended tasks.

```python
response = client.chat.completions.create(
    model="llama3.2:1B",
    messages=[
        {"role": "system", "content": "You are a creative brainstorming partner."},
        {"role": "user", "content": "Give me 5 startup ideas that combine AI and education."}
    ],
    temperature=0.9,      # More randomness
    top_p=0.95,           # Nucleus sampling
    presence_penalty=0.5, # Encourage novelty
)

print(response.choices[0].message.content)
```

---
