---
title: LLaMA
nav_order: 1
parent: Models
---

## üß© Model Card: Llama-3.2-1B-Instruct  

- **Type:** Text-to-Text
- **Think:** No  
- **Base Model:** [meta-llama/Llama-3.2-1B-Instruct](https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct)
- **Max Context Length (Model):** 128k tokens  
- **Default Context Length (FastFlowLM):** 128k tokens ([change default](https://docs.fastflowlm.com/instructions/cli.html))  

‚ñ∂Ô∏è Run with FastFlowLM in PowerShell:  

```powershell
flm run llama3.2:1b
```

---

## üß© Model Card: Llama-3.2-3B-Instruct  

- **Type:** Text-to-Text
- **Think:** No  
- **Base Model:** [meta-llama/Llama-3.2-3B-Instruct](https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct)
- **Max Context Length (Model):** 128k tokens  
- **Default Context Length (FastFlowLM):** 64k tokens ([change default](https://docs.fastflowlm.com/instructions/cli.html))  

‚ñ∂Ô∏è Run with FastFlowLM in PowerShell:  

```powershell
flm run llama3.2:3b
```

---

## üß© Model Card: Llama-3.1-8B-Instruct  

- **Type:** Text-to-Text
- **Think:** No  
- **Base Model:** [meta-llama/Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct)
- **Max Context Length (Model):** 128k tokens  
- **Default Context Length (FastFlowLM):** 16k tokens ([change default](https://docs.fastflowlm.com/instructions/cli.html))  

‚ñ∂Ô∏è Run with FastFlowLM in PowerShell:  

```powershell
flm run llama3.1:8b
```

---